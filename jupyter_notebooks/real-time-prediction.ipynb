{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "clear = lambda: os.system('clear')\n",
    "clear()\n",
    "import time\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import pvleopard as pv\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"../test_ds\"\n",
    "AUDIO_SUBFOLDER =\"leaders_ds\"\n",
    "DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "SHUFFLE_SEED = 43\n",
    "BATCH_SIZE = 128\n",
    "SCALE = 0.5\n",
    "filename = \"../test_ds/leaders_ds/Julia_Gillard/data.wav\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[31m[*]\\033[0m You will be asked to speak for few seconds for the recognition of the speaker.\")\n",
    "print(\"\\033[31m[*]\\033[0m Get Ready!\")\n",
    "time.sleep(5)\n",
    "\"\"\" Taking the voice input \"\"\"\n",
    "\n",
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 2\n",
    "fs = 16000  # Record at 16000 samples per second\n",
    "seconds = 6\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "print(\"\\033[31m[*]\\033[0m Recording\")\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "\t\t\t\tchannels=channels,\n",
    "\t\t\t\trate=fs,\n",
    "\t\t\t\tframes_per_buffer=chunk,\n",
    "\t\t\t\tinput=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "# Store data in chunks for 1 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "\tdata = stream.read(chunk)\n",
    "\tframes.append(data)\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print(\"\\033[31m[*]\\033[0m Finished recording\")\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waveforms and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(filename)\n",
    "# Plot waveform\n",
    "waveform_fig = go.Figure()\n",
    "waveform_fig.add_trace(go.Scatter(x=np.arange(len(y))/sr, y=y))\n",
    "waveform_fig.update_layout(title=\"Waveform\", xaxis_title=\"Time (seconds)\", yaxis_title=\"Amplitude\")\n",
    "pio.show(waveform_fig)\n",
    "\n",
    "# Plot Spectrogram\n",
    "D = librosa.stft(y)\n",
    "DB = librosa.amplitude_to_db(abs(D))\n",
    "\n",
    "spectrogram_fig = go.Figure()\n",
    "spectrogram_fig.add_trace(go.Heatmap(x=np.arange(DB.shape[1])/sr, y=np.arange(DB.shape[0]), z=DB))\n",
    "spectrogram_fig.update_layout(title=\"Spectrogram\", xaxis_title=\"Time (seconds)\", yaxis_title=\"Frequency (Hz)\")\n",
    "pio.show(spectrogram_fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech-To-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leopard = pv.create(access_key=config.access_key)\n",
    "\n",
    "transcript, words = leopard.process_file(filename)\n",
    "print(transcript)\n",
    "#for word in words:\n",
    "#    print(\n",
    "#      \"{word=\\\"%s\\\" start_sec=%.2f end_sec=%.2f confidence=%.2f}\"\n",
    "#      % (word.word, word.start_sec, word.end_sec, word.confidence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "\t\"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "\tpath_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "\taudio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "\tlabel_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\treturn tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "\t\"\"\"Reads and decodes an audio file.\"\"\"\n",
    "\taudio = tf.io.read_file(path)\n",
    "\taudio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "\treturn audio\n",
    "\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "\taudio = tf.squeeze(audio, axis=-1)\n",
    "\tfft = tf.signal.fft(\n",
    "\t\ttf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "\t)\n",
    "\tfft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "\t# Return the absolute value of the first half of the FFT\n",
    "\t# which represents the positive frequencies\n",
    "\treturn tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "\n",
    "def predict(path, labels):\n",
    "\ttest = paths_and_labels_to_dataset(path, labels)\n",
    "\n",
    "\ttest = test.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "\tBATCH_SIZE\n",
    "\t)\n",
    "\ttest = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\tfor audios, labels in test.take(1):\n",
    "\t\t# Get the signal FFT\n",
    "\t\tffts = audio_to_fft(audios)\n",
    "\t\t# Predict\n",
    "\t\ty_pred = model.predict(ffts)\n",
    "\t\t# Take random samples\n",
    "\t\t#rnd = np.random.randint(0, 3, 3)\n",
    "\t\taudios = audios.numpy()#[rnd, :]\n",
    "\t\tlabels = labels.numpy()#[rnd]\n",
    "\t\t#print(np.argmax(y_pred, axis=1))\n",
    "\t\ty_pred = np.argmax(y_pred, axis=-1)#[rnd]\n",
    "\n",
    "\t\tfor index in range(len(labels)):\n",
    "\t\t\tprint(\"\\033[31m[*]\\033[0m Model's prediction:\\33[92m {}\\33[0m\".format(trained_class_names[y_pred[index]]))\n",
    "\n",
    "\t\t\tif y_pred[index] == labels[index]:\n",
    "\t\t\t\tprint(\"\\033[31m[*]\\033[0m Correct! Belongs to:\\33[92m {}\\33[0m\".format(class_names[labels[index]]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"\\033[31m[*]\\033[0m Wrong! Belongs to:\\33[31m {}\\33[0m\".format(class_names[labels[index]]))\n",
    "\n",
    "\t\t\tdisplay(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "\n",
    "trained_class_names = os.listdir(\"../dataset/leaders_ds/audio\")\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "for label, name in enumerate(class_names):\n",
    "    #print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} speakers.\".format(len(audio_paths), len(np.unique(labels)))\n",
    ")\n",
    "\n",
    "\"\"\" Predict \"\"\"\n",
    "model = tf.keras.models.load_model('../models/leaders/v2/model.h5')\n",
    "predict(audio_paths, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
