{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "clear = lambda: os.system('clear')\n",
    "clear()\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import pvleopard as pv\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"../test_ds/leaders_ds\"\n",
    "NOISE_SUBFOLDER = \"noise\"\n",
    "AUDIO_SUBFOLDER =\"audio\"\n",
    "DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n",
    "DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "SHUFFLE_SEED = 43\n",
    "BATCH_SIZE = 128\n",
    "SCALE = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[31m[*]\\033[0m You will be asked to speak for few seconds for the recognition of the speaker.\")\n",
    "print(\"\\033[31m[*]\\033[0m Get Ready!\")\n",
    "time.sleep(5)\n",
    "\"\"\" Taking the voice input \"\"\"\n",
    "\n",
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 2\n",
    "fs = 16000  # Record at 16000 samples per second\n",
    "seconds = 6\n",
    "filename = \"../test_ds/leaders_ds/audio/Benjamin_Netanyahu/predict.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "# print(\"-------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[31m[*]\\033[0m Recording\")\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "\t\t\t\tchannels=channels,\n",
    "\t\t\t\trate=fs,\n",
    "\t\t\t\tframes_per_buffer=chunk,\n",
    "\t\t\t\tinput=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "# Store data in chunks for 1 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "\tdata = stream.read(chunk)\n",
    "\tframes.append(data)\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print(\"\\033[31m[*]\\033[0m Finished recording\")\n",
    "# print(\"-------------------------------------------------------------------------------------------\")\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waveforms and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(filename)\n",
    "# Plot waveform\n",
    "waveform_fig = go.Figure()\n",
    "waveform_fig.add_trace(go.Scatter(x=np.arange(len(y))/sr, y=y))\n",
    "waveform_fig.update_layout(title=\"Waveform\", xaxis_title=\"Time (seconds)\", yaxis_title=\"Amplitude\")\n",
    "pio.show(waveform_fig)\n",
    "\n",
    "# Plot Spectrogram\n",
    "D = librosa.stft(y)\n",
    "DB = librosa.amplitude_to_db(abs(D))\n",
    "\n",
    "spectrogram_fig = go.Figure()\n",
    "spectrogram_fig.add_trace(go.Heatmap(x=np.arange(DB.shape[1])/sr, y=np.arange(DB.shape[0]), z=DB))\n",
    "spectrogram_fig.update_layout(title=\"Spectrogram\", xaxis_title=\"Time (seconds)\", yaxis_title=\"Frequency (Hz)\")\n",
    "pio.show(spectrogram_fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech-To-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leopard = pv.create(access_key=config.access_key)\n",
    "\n",
    "transcript, words = leopard.process_file(filename)\n",
    "print(transcript)\n",
    "#for word in words:\n",
    "#    print(\n",
    "#      \"{word=\\\"%s\\\" start_sec=%.2f end_sec=%.2f confidence=%.2f}\"\n",
    "#      % (word.word, word.start_sec, word.end_sec, word.confidence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folder noise, does not exist, create it, otherwise do nothing\n",
    "if os.path.exists(DATASET_NOISE_PATH) is False:\n",
    "\tos.makedirs(DATASET_NOISE_PATH)\n",
    "\n",
    "for folder in os.listdir(DATASET_ROOT):\n",
    "\tif os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n",
    "\t\tif folder in [NOISE_SUBFOLDER]:\n",
    "\t\t\t# If folder is audio or noise, do nothing\n",
    "\t\t\tcontinue\n",
    "\t\telif folder in [\"other\", \"_background_noise_\"]:\n",
    "\t\t\t# If folder is one of the folders that contains noise samples move it to the noise folder\n",
    "\t\t\tshutil.move(\n",
    "\t\t\t\tos.path.join(DATASET_ROOT, folder),\n",
    "\t\t\t\tos.path.join(DATASET_NOISE_PATH, folder),\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "\n",
    "# Get the list of all noise files\n",
    "noise_paths = []\n",
    "for subdir in os.listdir(DATASET_NOISE_PATH):\n",
    "\tsubdir_path = Path(DATASET_NOISE_PATH) / subdir\n",
    "\tif os.path.isdir(subdir_path):\n",
    "\t\tnoise_paths += [\n",
    "\t\t\tos.path.join(subdir_path, filepath)\n",
    "\t\t\tfor filepath in os.listdir(subdir_path)\n",
    "\t\t\tif filepath.endswith(\".wav\")\n",
    "\t\t]\n",
    "\n",
    "print(\"Found {} files belonging to {} directories\".format(len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))))\n",
    "\n",
    "command = (\n",
    "\t#for /f \"delims=\" %d in ('dir /b /ad \"..\\test\\noise\"') do (for /f \"delims=\" %f in ('dir /b /a-d \"..\\test\\noise\\%d\\*.wav\"') do (ffprobe -hide_banner -loglevel panic -show_streams \"..\\dataset\\noise\\%d\\%f\" | findstr sample_rate | findstr /v 16000 && ffmpeg -hide_banner -loglevel panic -y -i \"..\\dataset\\noise\\%d\\%f\" -ar 16000 \"..\\dataset\\noise\\%d\\temp.wav\" && move /y \"..\\dataset\\noise\\%d\\temp.wav\" \"..\\dataset\\noise\\%d\\%f\"))\n",
    ")\n",
    "#os.system(command)\n",
    "\n",
    "# Split noise into chunks of 16,000 steps each\n",
    "def load_noise_sample(path):\n",
    "\tsample, sampling_rate = tf.audio.decode_wav(\n",
    "\t\ttf.io.read_file(path), desired_channels=1\n",
    "\t)\n",
    "\tif sampling_rate == SAMPLING_RATE:\n",
    "\t\t# Number of slices of 16000 each that can be generated from the noise sample\n",
    "\t\tslices = int(sample.shape[0] / SAMPLING_RATE)\n",
    "\t\tsample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
    "\t\treturn sample\n",
    "\telse:\n",
    "\t\tprint(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "noises = []\n",
    "for path in noise_paths:\n",
    "\tsample = load_noise_sample(path)\n",
    "\tif sample:\n",
    "\t\tnoises.extend(sample)\n",
    "noises = tf.stack(noises)\n",
    "\n",
    "print(\n",
    " \t\"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
    " \t\tlen(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
    " \t)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "\t\"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "\tpath_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "\taudio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "\tlabel_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\treturn tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "\t\"\"\"Reads and decodes an audio file.\"\"\"\n",
    "\taudio = tf.io.read_file(path)\n",
    "\taudio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "\treturn audio\n",
    "\n",
    "\n",
    "def add_noise(audio, noises=None, scale=0.5):\n",
    "\tif noises is not None:\n",
    "\t\t# Create a random tensor of the same size as audio ranging from\n",
    "\t\t# 0 to the number of noise stream samples that we have.\n",
    "\t\ttf_rnd = tf.random.uniform(\n",
    "\t\t\t(tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
    "\t\t)\n",
    "\t\tnoise = tf.gather(noises, tf_rnd, axis=0)\n",
    "\n",
    "\t\t# Get the amplitude proportion between the audio and the noise\n",
    "\t\tprop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
    "\t\tprop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
    "\n",
    "\t\t# Adding the rescaled noise to audio\n",
    "\t\taudio = audio + noise * prop * scale\n",
    "\treturn audio\n",
    "\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "\t# Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "\t# we need to squeeze the dimensions and then expand them again\n",
    "\t# after FFT\n",
    "\taudio = tf.squeeze(audio, axis=-1)\n",
    "\tfft = tf.signal.fft(\n",
    "\t\ttf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "\t)\n",
    "\tfft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "\t# Return the absolute value of the first half of the FFT\n",
    "\t# which represents the positive frequencies\n",
    "\treturn tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "\n",
    "def predict(path, labels):\n",
    "\ttest = paths_and_labels_to_dataset(path, labels)\n",
    "\n",
    "\n",
    "\ttest = test.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "\tBATCH_SIZE\n",
    "\t)\n",
    "\ttest = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\ttest = test.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
    "\n",
    "\tfor audios, labels in test.take(1):\n",
    "\t\t# Get the signal FFT\n",
    "\t\tffts = audio_to_fft(audios)\n",
    "\t\t# Predict\n",
    "\t\ty_pred = model.predict(ffts)\n",
    "\t\t# Take random samples\n",
    "\t\t#rnd = np.random.randint(0, 3, 3)\n",
    "\t\taudios = audios.numpy()#[rnd, :]\n",
    "\t\tlabels = labels.numpy()#[rnd]\n",
    "\t\t#print(np.argmax(y_pred, axis=1))\n",
    "\t\ty_pred = np.argmax(y_pred, axis=-1)#[rnd]\n",
    "\n",
    "\t\tfor index in range(len(labels)):\n",
    "\t\t\tprint(\"\\033[31m[*]\\033[0m Model's prediction:\\33[92m {}\\33[0m\".format(trained_class_names[y_pred[index]]))\n",
    "\n",
    "\t\t\tif y_pred[index] == labels[index]:\n",
    "\t\t\t\tprint(\"\\033[31m[*]\\033[0m Correct! Belongs to:\\33[92m {}\\33[0m\".format(class_names[labels[index]]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"\\033[31m[*]\\033[0m Wrong! Belongs to:\\33[31m {}\\33[0m\".format(class_names[labels[index]]))\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\t\tdisplay(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "\n",
    "trained_class_names = os.listdir(\"../dataset/leaders_ds/audio\")\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "for label, name in enumerate(class_names):\n",
    "    #print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} speakers.\".format(len(audio_paths), len(np.unique(labels)))\n",
    ")\n",
    "\n",
    "\"\"\" Predict \"\"\"\n",
    "model = tf.keras.models.load_model('../models/leaders/model.h5')\n",
    "predict(audio_paths, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
